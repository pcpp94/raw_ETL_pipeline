{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "from src.config import COMPILED_OUTPUTS_DIR, OUTPUTS_DIR, DATA_DIR, normalise_dict, normalise_dict_fc, MISSING_DIR\n",
        "\n",
        "actuals_data = os.path.join(DATA_DIR,'pcweather_f_actuals.csv')\n",
        "missing_data1 = os.path.join(MISSING_DIR,'hourly_weather_half_hourly_weather_stations_formatted.parquet')\n",
        "missing_data2 = os.path.join(MISSING_DIR,'half_hourly_weather_stations_formatted.parquet')\n",
        "missing_data3 = os.path.join(MISSING_DIR,'pcweather_f_forecasts_archive_2.parquet')\n",
        "forecast_data = os.path.join(DATA_DIR,'pcweather_f_forecasts.csv')\n",
        "actuals_api = [os.path.join(OUTPUTS_DIR,file) for file in os.listdir(OUTPUTS_DIR) if file.startswith('actual')]\n",
        "forecast_api = [os.path.join(OUTPUTS_DIR,file) for file in os.listdir(OUTPUTS_DIR) if file.startswith('forecast')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtype = { \n",
        "    'hour': str,\n",
        "    'type': str,\n",
        "    'weather_station': str,\n",
        "    'station_id': int,\n",
        "    'weather_variable': str,\n",
        "    'nominal': float,}\n",
        "df = pd.read_csv(forecast_data, index_col=0, parse_dates=['datetime', 'date', 'file_date'], dtype=dtype).drop(columns=['last_forecast']).rename(columns={'file_date' : 'file_datetime'})\n",
        "df['weather_station'] = df['weather_station'].apply(lambda x: normalise_dict_fc[x][0])\n",
        "\n",
        "df_2 = pd.DataFrame()\n",
        "dtype_2 = { \n",
        "    'hour': str,\n",
        "    'type': str,\n",
        "    'weather_station': str,\n",
        "    'station_id': int,\n",
        "    'weather_variable': str,\n",
        "    'nominal': float,}\n",
        "for file in forecast_api:\n",
        "    try:\n",
        "        aux = pd.read_csv(file, index_col=0, parse_dates=['datetime', 'date', 'file_datetime']).drop(columns=['weather_station_api'])\n",
        "    except:\n",
        "        aux = pd.read_csv(file, index_col=0, parse_dates=['datetime', 'date', 'file_datetime'])\n",
        "    aux['weather_variable'] = aux['weather_variable'].str.lower()\n",
        "    if ((\"temp\" in aux['weather_variable'].unique()) or (\"rh\" in aux['weather_variable'].unique())):\n",
        "        aux['weather_variable'] = aux['weather_variable'].map({\n",
        "            'temp' : 'temperature',\n",
        "            'rh' : 'humidity',\n",
        "            'rain' : 'rain'\n",
        "        })\n",
        "    df_2 = pd.concat([df_2,aux]).reset_index(drop=True)\n",
        "df_2['weather_station'] = df_2['weather_station'].apply(lambda x: 'Lima I.A.' if x == 'Jorge Chavez I.A.' else x)\n",
        "try:\n",
        "    df_2 = df_2.drop(columns='id')\n",
        "except:\n",
        "    pass\n",
        "df_2 = df_2.drop_duplicates()\n",
        "df = pd.concat([df, df_2]).drop_duplicates().reset_index(drop=True)\n",
        "current_files = df['file_datetime'].dt.date.unique().tolist()\n",
        "\n",
        "df_3 = pd.DataFrame()\n",
        "aux = pd.read_parquet(missing_data3).drop(columns=['file'])\n",
        "for weather in aux['weather_variable'].unique():\n",
        "    aux1 = aux[aux['weather_variable'] == weather].copy()\n",
        "    aux1 = aux1[~aux1['nominal'].isna()]\n",
        "    df_3 = pd.concat([df_3, aux1])\n",
        "df_3 = df_3.reset_index(drop=True)\n",
        "archives = df_3['file_datetime'].dt.date.unique().tolist()\n",
        "to_add = [x for x in archives if x not in current_files]\n",
        "df_3 = df_3[df_3['file_datetime'].dt.date.isin(to_add)]\n",
        "\n",
        "df = pd.concat([df, df_3]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "df['hour'] = df['hour'].str.replace(\":03\", \":30\")\n",
        "df['datetime'] = pd.to_datetime(df['date'].astype(str) + \" \" + df['hour'].astype(str), format=\"%Y-%m-%d %H:%M\")\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "\n",
        "last_fc = df.groupby(by=['datetime','type', 'weather_station', 'weather_variable']).max()[['file_datetime']].reset_index().rename(columns={'file_datetime' : 'last_file_date'})\n",
        "df = df.merge(last_fc, how='left', on=['datetime','type', 'weather_station', 'weather_variable'])\n",
        "df['last_fc'] = df['file_datetime'] == df['last_file_date']\n",
        "df = df.drop(columns=['last_file_date'])\n",
        "station_ids_temp = dict(df[~df['station_id'].isna()][['weather_station', 'station_id']].drop_duplicates().values)\n",
        "df['station_id'] = df['weather_station'].map(station_ids_temp)\n",
        "df['id'] = df['weather_station'] + df['weather_variable'] + df['datetime'].astype(str) + df['nominal'].astype(str)\n",
        "df = df.groupby(by=['id', 'datetime', 'date', 'hour', 'type', 'weather_station', 'weather_variable', 'nominal', 'station_id', 'last_fc'])['file_datetime'].max().reset_index().drop(columns='id')\n",
        "\n",
        "df['forecast_recency'] = df.groupby(['datetime', 'date', 'hour', 'type', 'weather_station', 'weather_variable'])['file_datetime'].rank(method='dense', ascending=False).astype(int)\n",
        "df['forecast_days_ap'] = df['datetime'].dt.date - df['file_datetime'].dt.date\n",
        "df['forecast_days_ap'] = df['forecast_days_ap'].apply(lambda x: x.days if x.days >=0 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I need to add the hours correction to the run_etl_pipeline.\n",
        "\n",
        "\n",
        "df['hour'] = df['hour'].str.replace(\":03\", \":30\")\n",
        "df['datetime'] = pd.to_datetime(df['date'].astype(str) + \" \" + df['hour'].astype(str), format=\"%Y-%m-%d %H:%M\")\n",
        "df = df.drop_duplicates()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pablo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
